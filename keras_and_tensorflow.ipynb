{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with keras and tensorflow\n",
    "\n",
    "N.B. You will need to pip install keras and tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we'll use sklearn's built-in breast cancer dataset. The next cell loads the data and prints the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting our data and initializing a Scaler\n",
    "X_train,X_test,y_train,y_test = train_test_split(data.data,data.target)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "#the neural network relies on linear transformations, so scaling is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming our data\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "X_train_s.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.57359484e+00,  2.47316288e+00,  1.34294416e+00, -3.93090963e-01,\n",
       "       -4.91266742e-01,  4.03191838e-02, -8.26501110e-01,  1.07475666e+00,\n",
       "        2.01101836e+00,  1.87684030e-02,  7.08393388e-01, -1.29822376e+00,\n",
       "        1.19338713e-01,  3.10901209e-01,  1.74412931e-01, -6.56489395e-01,\n",
       "        5.55143391e-01,  1.06996760e+00, -1.58077844e+00, -1.20483704e+00,\n",
       "        1.69733477e+00,  6.90535581e-02, -1.10536282e-01, -1.15694641e+00,\n",
       "       -8.91153452e-01, -1.25751673e+00,  1.27350275e+00,  3.55301214e-02,\n",
       "        1.16944182e-01, -1.87161280e-01, -5.48735491e-01,  1.18729963e+00,\n",
       "       -5.29579241e-01, -7.57059705e-01,  2.53432460e-01, -5.10422992e-01,\n",
       "       -9.15098764e-01,  9.11928540e-01, -8.76786265e-01, -2.09799718e+00,\n",
       "       -5.22395648e-01, -1.43950110e+00, -3.87846602e-04, -1.02285267e+00,\n",
       "        1.06757307e+00, -6.06204240e-01, -1.91950342e-01, -8.28895641e-01,\n",
       "        5.95850422e-01,  6.55713702e-01,  3.01323084e-01,  1.07715119e+00,\n",
       "        1.08672932e+00,  7.85018386e-01, -8.16922985e-01, -2.75758934e-01,\n",
       "        2.36780351e+00, -1.14257923e+00, -5.96626115e-01, -1.37963782e+00,\n",
       "       -4.60137837e-01,  5.00069174e-01,  9.77879325e-02,  2.78684647e+00,\n",
       "        2.63010585e-01,  2.91744959e-01,  9.06043389e-02,  3.06112147e-01,\n",
       "        4.73729330e-01,  1.26522307e-01,  6.19795734e-01,  1.36928400e+00,\n",
       "        1.06996760e+00, -1.39400500e+00, -5.17690900e-03, -1.20723157e+00,\n",
       "       -2.78153466e-01,  5.31198079e-01, -8.38473766e-01,  4.23444175e-01,\n",
       "        2.26004960e+00, -7.97766735e-01, -4.69715961e-01, -1.68613781e+00,\n",
       "        3.11010818e+00,  3.37111208e+00, -5.20001116e-01, -4.57743305e-01,\n",
       "       -2.85337059e-01, -5.34368304e-01, -6.73251113e-01, -9.13800321e-02,\n",
       "        3.20479334e-01, -3.16465965e-01, -3.42805808e-01, -3.93090963e-01,\n",
       "       -6.30149552e-01, -2.70969872e-01,  1.19927228e+00, -9.62989388e-01,\n",
       "        2.15119961e-01, -2.82942528e-01, -8.48051891e-01, -6.94801894e-01,\n",
       "        1.94636602e+00,  2.05411992e+00,  6.55713702e-01, -1.29692531e-01,\n",
       "        5.79088703e-01, -8.89855009e-02,  5.88666828e-01, -3.71540182e-01,\n",
       "       -9.53411263e-01, -1.33653625e+00, -1.58426906e-01,  1.62549884e+00,\n",
       "       -2.58997216e-01, -7.54665174e-01, -9.51016732e-01, -7.83399548e-01,\n",
       "       -1.44907922e+00,  1.38604572e+00, -4.33797993e-01, -9.51016732e-01,\n",
       "        4.04287926e-01, -7.40297987e-01, -6.37333145e-01,  5.64721516e-01,\n",
       "       -7.92977673e-01, -1.21680969e+00,  4.51082461e-02, -2.68575341e-01,\n",
       "       -1.08271595e+00,  1.19338713e-01,  4.66545737e-01, -3.39112834e-02,\n",
       "       -7.04380019e-01,  1.81466680e+00, -3.81118307e-01, -1.39270656e-01,\n",
       "       -6.06204240e-01, -1.10666126e+00,  4.25838707e-01, -1.10187220e+00,\n",
       "        6.79574699e-03,  6.38951983e-01, -1.04679798e+00, -5.43946428e-01,\n",
       "       -1.03721986e+00,  4.23444175e-01,  4.30627769e-01,  3.31355902e-02,\n",
       "       -1.51373156e+00, -1.08271595e+00,  4.21049644e-01,  3.63580895e-01,\n",
       "       -3.38016745e-01, -3.30833152e-01,  3.68369958e-01, -1.05747219e-01,\n",
       "       -1.44668469e+00,  8.34207453e-02, -6.26456577e-02, -4.31403462e-01,\n",
       "       -7.95372204e-01, -3.63058146e-02, -1.06355970e+00, -1.13060657e+00,\n",
       "       -1.59035656e+00, -7.16352675e-01, -1.39270656e-01, -9.05520639e-01,\n",
       "       -1.62148547e+00,  6.74869951e-01,  2.51037929e-01,  1.35012775e+00,\n",
       "        3.22873865e-01, -5.29579241e-01,  3.30057459e-01, -5.75075334e-01,\n",
       "       -4.36192525e-01,  1.31899884e+00, -7.92977673e-01,  5.26409017e-01,\n",
       "        5.07252767e-01,  7.32338700e-01,  6.70080889e-01,  1.24476838e+00,\n",
       "        1.35252228e+00, -4.84083148e-01, -1.53767688e+00, -3.14071433e-01,\n",
       "        1.31660431e+00,  5.50354329e-01, -3.47594870e-01, -1.34611438e+00,\n",
       "        2.09961601e+00,  1.63028790e+00,  6.72475420e-01,  1.37407306e+00,\n",
       "        9.06043389e-02,  3.61186364e-01,  8.92772290e-01, -1.51133703e+00,\n",
       "        8.34207453e-02, -2.47024560e-01,  1.69623868e-01, -1.70399562e-01,\n",
       "        1.34773322e+00, -3.14071433e-01, -2.42235498e-01, -1.12581751e+00,\n",
       "       -2.17941124e+00,  2.09482695e+00, -8.67208140e-01,  2.58221523e-01,\n",
       "        1.27589728e+00, -2.78153466e-01, -1.31738001e+00,  8.71221509e-01,\n",
       "       -1.87161280e-01, -2.97309715e-01,  6.82053545e-01, -8.65909697e-02,\n",
       "       -1.46344641e+00,  1.07475666e+00, -2.25473779e-01,  7.08393388e-01,\n",
       "       -7.37903455e-01, -3.40411277e-01,  7.65862137e-01,  1.67338946e+00,\n",
       "        1.43872540e+00, -1.32935266e+00, -9.07915170e-01,  8.52065260e-01,\n",
       "        7.49100418e-01, -1.28625110e+00, -1.41665187e-01, -7.54665174e-01,\n",
       "        1.76807462e-01,  1.13222541e+00,  5.04858236e-01,  5.71905110e-01,\n",
       "        1.29984259e+00, -1.25033313e+00, -9.05520639e-01, -2.54208154e-01,\n",
       "       -3.09282371e-01,  4.37811363e-01,  1.09760588e-01,  1.86385587e-01,\n",
       "       -2.15895654e-01,  4.54573081e-01, -3.11676902e-01, -6.10993302e-01,\n",
       "        1.45548712e+00, -6.73251113e-01,  6.29373858e-01, -3.21255027e-01,\n",
       "        2.69106522e+00,  4.90491049e-01, -1.69811047e+00,  9.21506664e-01,\n",
       "       -3.49989401e-01,  1.67229337e-01,  1.86385587e-01, -8.83969858e-01,\n",
       "        1.98358243e-01, -1.16173548e+00,  2.09003789e+00, -2.08712061e-01,\n",
       "       -1.56162219e+00,  1.72018400e-01,  2.04693633e+00, -1.14257923e+00,\n",
       "        5.57537922e-01,  3.06112147e-01,  3.58791833e-01, -8.74391734e-01,\n",
       "       -1.61430187e+00, -8.41964385e-02,  8.34207453e-02, -5.77469865e-01,\n",
       "        5.04858236e-01,  2.94139491e-01,  4.97674642e-01, -9.79751106e-01,\n",
       "        9.62213695e-01,  1.72128008e+00, -1.08032142e+00,  8.90377759e-01,\n",
       "       -1.03003626e+00,  1.42196368e+00,  1.70451837e+00, -1.46454250e-01,\n",
       "       -4.67321430e-01, -8.28895641e-01, -9.29465951e-01, -1.08989954e+00,\n",
       "        1.79311602e+00, -7.09169081e-01, -2.43331586e-02, -1.36527063e+00,\n",
       "       -5.15212054e-01,  1.39793406e-02, -1.39270656e-01,  6.07823078e-01,\n",
       "       -4.79294086e-01, -1.36876125e-01,  5.35987142e-01, -6.70856582e-01,\n",
       "       -1.75188624e-01,  5.02463705e-01,  5.59932454e-01, -7.83399548e-01,\n",
       "        9.76580882e-01, -1.48260266e+00,  1.02686604e+00, -7.49876111e-01,\n",
       "       -5.46340960e-01, -2.37446435e-01, -4.79294086e-01, -1.49696985e+00,\n",
       "        1.72018400e-01, -1.08989954e+00, -1.29582922e+00, -4.84083148e-01,\n",
       "       -5.08028460e-01,  1.55256681e-01,  1.75959258e+00,  7.01209794e-01,\n",
       "        2.83465278e-02,  4.78518393e-01, -8.16922985e-01,  7.34733231e-01,\n",
       "       -1.08511048e+00, -9.61690945e-02,  3.01323084e-01,  4.85701986e-01,\n",
       "        5.74299641e-01,  4.73729330e-01,  2.91744959e-01,  1.76677618e+00,\n",
       "       -1.03003626e+00, -3.83512838e-01,  8.92772290e-01, -1.96739405e-01,\n",
       "       -6.30149552e-01,  1.83621758e+00,  1.24716291e+00, -4.58839394e-02,\n",
       "        4.52178550e-01,  1.39793406e-02, -1.27427844e+00, -3.73934713e-01,\n",
       "        1.22321760e+00, -1.05747219e-01, -1.70399562e-01, -5.94231584e-01,\n",
       "        1.04362776e+00, -1.52809875e+00,  1.06517854e+00, -6.30149552e-01,\n",
       "        2.84561366e-01,  1.21124494e+00,  3.65975427e-01,  3.43097536e+00,\n",
       "        1.32139337e+00, -2.11106592e-01, -9.85636257e-02,  4.74078392e+00,\n",
       "       -3.18860496e-01, -4.40981587e-01, -1.73881750e+00, -4.10948770e-02,\n",
       "       -4.09852681e-01, -5.82258928e-01,  1.29505353e+00, -1.08989954e+00,\n",
       "       -1.03721986e+00, -9.17493295e-01,  5.07252767e-01, -2.30262842e-01,\n",
       "       -2.05250108e+00, -2.33984483e+00, -1.41665187e-01, -6.08598771e-01,\n",
       "        1.14659260e+00,  6.34162921e-01,  5.64721516e-01,  8.56854322e-01,\n",
       "        8.49670729e-01, -4.45770649e-01,  2.82166835e-01,  1.28916838e-01,\n",
       "        4.61756675e-01, -6.32544083e-01,  1.76807462e-01, -2.11106592e-01,\n",
       "       -6.32544083e-01,  6.96420732e-01, -6.85223769e-01,  5.81483234e-01,\n",
       "       -1.51852063e+00,  2.35822538e+00, -7.90583142e-01, -9.85636257e-02,\n",
       "       -3.15167522e-02,  7.29944169e-01,  9.11928540e-01,  1.03644416e+00,\n",
       "        8.10262141e-02,  1.49858868e+00, -8.55235484e-01,  1.02207697e+00,\n",
       "       -3.02098777e-01, -1.45865735e+00, -1.81783703e+00, -6.94801894e-01,\n",
       "       -3.02098777e-01, -1.29343469e+00, -1.53288781e+00,  1.81945586e+00,\n",
       "       -1.95440962e-02, -1.53637843e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing model and layer types\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Import the optimizer\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing and compiling our model\n",
    "model = Sequential() #takes layers as an arguement\n",
    "\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "hiddens = inputs\n",
    "\n",
    "model.add(Dense(hiddens, input_dim=inputs,activation='relu')) #model.add(layers)\n",
    "model.add(Dense(1))\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(optimizer=adam,loss='mean_squared_error')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/50\n",
      "426/426 [==============================] - 0s 1ms/step - loss: 1.8794 - val_loss: 1.5582\n",
      "Epoch 2/50\n",
      "426/426 [==============================] - 0s 104us/step - loss: 0.8217 - val_loss: 0.7690\n",
      "Epoch 3/50\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.4387 - val_loss: 0.4544\n",
      "Epoch 4/50\n",
      "426/426 [==============================] - 0s 97us/step - loss: 0.2976 - val_loss: 0.3459\n",
      "Epoch 5/50\n",
      "426/426 [==============================] - 0s 166us/step - loss: 0.2363 - val_loss: 0.2854\n",
      "Epoch 6/50\n",
      "426/426 [==============================] - 0s 98us/step - loss: 0.1935 - val_loss: 0.2437\n",
      "Epoch 7/50\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.1670 - val_loss: 0.2180\n",
      "Epoch 8/50\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.1477 - val_loss: 0.1975\n",
      "Epoch 9/50\n",
      "426/426 [==============================] - 0s 105us/step - loss: 0.1336 - val_loss: 0.1829\n",
      "Epoch 10/50\n",
      "426/426 [==============================] - 0s 140us/step - loss: 0.1234 - val_loss: 0.1710\n",
      "Epoch 11/50\n",
      "426/426 [==============================] - 0s 241us/step - loss: 0.1146 - val_loss: 0.1590\n",
      "Epoch 12/50\n",
      "426/426 [==============================] - 0s 225us/step - loss: 0.1078 - val_loss: 0.1505\n",
      "Epoch 13/50\n",
      "426/426 [==============================] - 0s 243us/step - loss: 0.1013 - val_loss: 0.1432\n",
      "Epoch 14/50\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0969 - val_loss: 0.1376\n",
      "Epoch 15/50\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0927 - val_loss: 0.1305\n",
      "Epoch 16/50\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0886 - val_loss: 0.1261\n",
      "Epoch 17/50\n",
      "426/426 [==============================] - 0s 107us/step - loss: 0.0869 - val_loss: 0.1213\n",
      "Epoch 18/50\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0821 - val_loss: 0.1170\n",
      "Epoch 19/50\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0797 - val_loss: 0.1115\n",
      "Epoch 20/50\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0765 - val_loss: 0.1108\n",
      "Epoch 21/50\n",
      "426/426 [==============================] - 0s 134us/step - loss: 0.0744 - val_loss: 0.1078\n",
      "Epoch 22/50\n",
      "426/426 [==============================] - 0s 122us/step - loss: 0.0723 - val_loss: 0.1049\n",
      "Epoch 23/50\n",
      "426/426 [==============================] - 0s 149us/step - loss: 0.0703 - val_loss: 0.1014\n",
      "Epoch 24/50\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0683 - val_loss: 0.1009\n",
      "Epoch 25/50\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0668 - val_loss: 0.0990\n",
      "Epoch 26/50\n",
      "426/426 [==============================] - 0s 125us/step - loss: 0.0654 - val_loss: 0.0943\n",
      "Epoch 27/50\n",
      "426/426 [==============================] - 0s 100us/step - loss: 0.0634 - val_loss: 0.0936\n",
      "Epoch 28/50\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0625 - val_loss: 0.0926\n",
      "Epoch 29/50\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0604 - val_loss: 0.0907\n",
      "Epoch 30/50\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0594 - val_loss: 0.0892\n",
      "Epoch 31/50\n",
      "426/426 [==============================] - 0s 115us/step - loss: 0.0577 - val_loss: 0.0879\n",
      "Epoch 32/50\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0574 - val_loss: 0.0863\n",
      "Epoch 33/50\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0559 - val_loss: 0.0866\n",
      "Epoch 34/50\n",
      "426/426 [==============================] - 0s 55us/step - loss: 0.0545 - val_loss: 0.0840\n",
      "Epoch 35/50\n",
      "426/426 [==============================] - 0s 51us/step - loss: 0.0540 - val_loss: 0.0832\n",
      "Epoch 36/50\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0528 - val_loss: 0.0830\n",
      "Epoch 37/50\n",
      "426/426 [==============================] - 0s 108us/step - loss: 0.0522 - val_loss: 0.0834\n",
      "Epoch 38/50\n",
      "426/426 [==============================] - 0s 57us/step - loss: 0.0510 - val_loss: 0.0813\n",
      "Epoch 39/50\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0506 - val_loss: 0.0791\n",
      "Epoch 40/50\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0501 - val_loss: 0.0795\n",
      "Epoch 41/50\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0501 - val_loss: 0.0788\n",
      "Epoch 42/50\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0484 - val_loss: 0.0783\n",
      "Epoch 43/50\n",
      "426/426 [==============================] - 0s 119us/step - loss: 0.0470 - val_loss: 0.0765\n",
      "Epoch 44/50\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0479 - val_loss: 0.0755\n",
      "Epoch 45/50\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0462 - val_loss: 0.0782\n",
      "Epoch 46/50\n",
      "426/426 [==============================] - 0s 106us/step - loss: 0.0452 - val_loss: 0.0762\n",
      "Epoch 47/50\n",
      "426/426 [==============================] - 0s 122us/step - loss: 0.0444 - val_loss: 0.0744\n",
      "Epoch 48/50\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0441 - val_loss: 0.0747\n",
      "Epoch 49/50\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0435 - val_loss: 0.0742\n",
      "Epoch 50/50\n",
      "426/426 [==============================] - 0s 128us/step - loss: 0.0429 - val_loss: 0.0730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a283332e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "\n",
    "model.fit(X_train_s,y_train,\n",
    "            validation_data = (X_test_s,y_test), epochs=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing that fit as a history log\n",
    "\n",
    "history_log = model.fit(X_train_s,y_train,\n",
    "            validation_data = (X_test_s,y_test), epochs=50\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our losses\n",
    "train_loss = history_log.history['loss']\n",
    "test_loss = history_log.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, label= 'Training Loss')\n",
    "plt.plot(test_loss, label=\"Testing Loss\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=3\n",
    "b=5\n",
    "c=a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow as a graph constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the graph\n",
    "a = tf.Variable(4)\n",
    "b = tf.Variable(5)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "d = a + c * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = sess.run(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the output\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 30))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "hid = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "y_hat = tf.layers.dense(hid, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train_s, y: y_train.reshape(-1, 1)})\n",
    "        \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test_s})\n",
    "\n",
    "classes = (pred > 0.5).astype(int)\n",
    "\n",
    "metrics.accuracy_score(y_test.reshape(-1, 1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
